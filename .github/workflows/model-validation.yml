name: Model Validation and Monitoring

on:
  schedule:
    # Run daily at 2 AM
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  validate-models:
    name: Validate Model Performance
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r ai-service/requirements.txt
      
      - name: Download latest models
        uses: actions/download-artifact@v3
        with:
          name: trained-models
          path: ai-service/models/
        continue-on-error: true
      
      - name: Generate validation data
        working-directory: ./ai-service
        run: |
          python -c "
          from data_generator import IndustrialDataGenerator
          gen = IndustrialDataGenerator(seed=999)  # Different seed for validation
          gen.generate_ml_training_data(output_dir='./validation_data', train_split=0.5)
          "
      
      - name: Validate model performance
        working-directory: ./ai-service
        run: |
          python -c "
          import pandas as pd
          import numpy as np
          from model_inference import ModelInference
          import json
          
          print('\n' + '='*60)
          print('üîç MODEL VALIDATION REPORT')
          print('='*60)
          
          inf = ModelInference(model_dir='./models')
          
          # Load validation data
          df = pd.read_csv('./validation_data/test_data.csv')
          
          validation_results = {
            'timestamp': pd.Timestamp.now().isoformat(),
            'tests': {}
          }
          
          # Test 1: Forecast consistency
          print('\nüìà Testing Forecast Consistency...')
          try:
            historical = df['power'].head(24).tolist()
            result = inf.forecast_energy(historical, periods=12)
            all_positive = all(x > 0 for x in result['forecast'])
            within_bounds = all(
              result['lower_bound'][i] <= result['forecast'][i] <= result['upper_bound'][i]
              for i in range(len(result['forecast']))
            )
            validation_results['tests']['forecast'] = {
              'passed': all_positive and within_bounds,
              'checks': {
                'all_positive': all_positive,
                'within_bounds': within_bounds
              }
            }
            print(f'  ‚úì Forecast values positive: {all_positive}')
            print(f'  ‚úì Within confidence bounds: {within_bounds}')
          except Exception as e:
            print(f'  ‚ùå Forecast test failed: {e}')
            validation_results['tests']['forecast'] = {'passed': False, 'error': str(e)}
          
          # Test 2: Anomaly detection sensitivity
          print('\nüéØ Testing Anomaly Detection...')
          try:
            normal_score = inf.detect_anomalies(100, 45, 2, 1, 5)['anomaly_score']
            abnormal_score = inf.detect_anomalies(250, 85, 8, 0.5, 2)['anomaly_score']
            sensitivity = abnormal_score > normal_score
            validation_results['tests']['anomaly'] = {
              'passed': sensitivity,
              'normal_score': normal_score,
              'abnormal_score': abnormal_score,
              'sensitivity': sensitivity
            }
            print(f'  ‚úì Normal case score: {normal_score:.3f}')
            print(f'  ‚úì Abnormal case score: {abnormal_score:.3f}')
            print(f'  ‚úì Model is sensitive: {sensitivity}')
          except Exception as e:
            print(f'  ‚ùå Anomaly test failed: {e}')
            validation_results['tests']['anomaly'] = {'passed': False, 'error': str(e)}
          
          # Test 3: Recommendation consistency
          print('\nüîß Testing Recommendations...')
          try:
            low_risk = inf.recommend_maintenance(100, 45, 2)
            high_risk = inf.recommend_maintenance(280, 95, 9)
            is_consistent = low_risk['risk_level'] <= high_risk['risk_level']
            validation_results['tests']['recommendation'] = {
              'passed': is_consistent,
              'low_risk_level': low_risk['risk_level'],
              'high_risk_level': high_risk['risk_level'],
              'consistency': is_consistent
            }
            print(f'  ‚úì Low risk recommendation: level={low_risk[\"risk_level\"]}')
            print(f'  ‚úì High risk recommendation: level={high_risk[\"risk_level\"]}')
            print(f'  ‚úì Models are consistent: {is_consistent}')
          except Exception as e:
            print(f'  ‚ùå Recommendation test failed: {e}')
            validation_results['tests']['recommendation'] = {'passed': False, 'error': str(e)}
          
          # Summary
          all_passed = all(v.get('passed', False) for v in validation_results['tests'].values())
          print('\n' + '='*60)
          if all_passed:
            print('‚úÖ ALL VALIDATION TESTS PASSED')
          else:
            print('‚ö†Ô∏è  SOME VALIDATION TESTS FAILED')
          print('='*60 + '\n')
          
          # Save report
          with open('./validation_report.json', 'w') as f:
            json.dump(validation_results, f, indent=2)
          "
      
      - name: Upload validation report
        uses: actions/upload-artifact@v3
        with:
          name: validation-report
          path: ai-service/validation_report.json
        continue-on-error: true

  model-registry:
    name: Update Model Registry
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Create model metadata
        run: |
          cat > MODEL_REGISTRY.md << 'EOF'
          # Model Registry
          
          ## Current Models
          
          ### 1. Forecast Model (Prophet)
          - **Type**: Time Series Forecasting
          - **Framework**: Facebook Prophet
          - **Input**: Historical power consumption (hourly)
          - **Output**: 24-hour or 7-day forecast with confidence intervals
          - **Use Case**: Energy demand forecasting, capacity planning
          - **Retraining**: Weekly
          
          ### 2. Anomaly Detection (Isolation Forest)
          - **Type**: Unsupervised Anomaly Detection
          - **Framework**: Scikit-Learn
          - **Features**: Power, Temperature, Vibration, Runtime, Production
          - **Output**: Anomaly score (0-1), Boolean anomaly flag
          - **Use Case**: Real-time equipment anomaly detection
          - **Retraining**: Weekly
          
          ### 3. Maintenance Recommendation (Random Forest)
          - **Type**: Risk Level Prediction
          - **Framework**: Scikit-Learn
          - **Input**: Equipment sensor metrics
          - **Output**: Risk level (0-3), Maintenance urgency
          - **Use Case**: Predictive maintenance scheduling
          - **Retraining**: Weekly
          
          ## Pipeline
          1. **Data Generation** (Weekly)
             - Generates 180 days of synthetic industrial data
             - Includes normal operations and anomalies
          
          2. **Model Training** (Weekly)
             - Trains all 3 models on generated data
             - Evaluates performance on test set
          
          3. **Validation** (Daily)
             - Tests model consistency and performance
             - Validates output ranges and formats
          
          4. **Integration** (On Deploy)
             - Models packaged in Docker image
             - Integrated with FastAPI service
             - Loaded on service startup
          
          ## Endpoints
          
          ### POST /forecast
          Request:
          ```json
          {
            "data": [100, 101, 99, 102, ...],
            "horizon": 24
          }
          ```
          Response:
          ```json
          {
            "forecast": [...],
            "lower_bound": [...],
            "upper_bound": [...],
            "model": "prophet"
          }
          ```
          
          ### POST /anomaly
          Request:
          ```json
          {
            "data": [
              {"power": 100, "temperature": 45, "vibration": 2}
            ]
          }
          ```
          Response:
          ```json
          {
            "results": [
              {
                "is_anomaly": false,
                "anomaly_score": 0.25,
                "model": "isolation_forest"
              }
            ]
          }
          ```
          
          ### POST /recommendations
          Request:
          ```json
          {
            "data": [
              {"power": 100, "temperature": 45, "vibration": 2}
            ]
          }
          ```
          Response:
          ```json
          {
            "results": [
              {
                "risk_level": 0,
                "urgency": "NONE",
                "recommendation": "Normal operation",
                "model": "random_forest"
              }
            ]
          }
          ```
          
          ## Model Performance
          
          | Model | Metric | Score |
          |-------|--------|-------|
          | Forecast | RMSE | < 15 kW |
          | Forecast | MAPE | < 10% |
          | Anomaly | F1-Score | > 0.85 |
          | Anomaly | Precision | > 0.80 |
          | Recommendation | R¬≤ | > 0.75 |
          
          ## Deployment
          
          Models are automatically:
          1. Trained on schedule
          2. Validated for quality
          3. Packaged in Docker images
          4. Deployed to production
          
          Version: 1.0.0
          Last Updated: $(date)
          EOF
      
      - name: Commit registry
        run: |
          git config user.name "GitHub Action"
          git config user.email "action@github.com"
          if [ "$(git status -s)" != "" ]; then
            git add MODEL_REGISTRY.md
            git commit -m "docs: Update model registry [skip ci]"
            git push
          fi
        continue-on-error: true
